+++
paginate_by = 0
sort_by = "date"
+++

{% note(title="**⚠️ On the job market**") %}
I am on the faculty job market this fall! I am primarily looking for **electrical and/or computer science departments** with collaborations in **neuroscience**. If my research interests you, please reach out!
{% end %}

Welcome! I'm Kyle, a NeuroAI Scholar at Cold Spring Harbor Lab in Long Island, NY. I completed my PhD studying non-von Neumman computing under Dr. Mikko Lipasti at University of Wisconsin-Madison. My research is broadly organized into three areas:
1. <img src="/cheese-3d.gif" style="width:150px; margin-left: 2%; border-radius: 5px; float:right">

   **Applying ML models to neuroscience datasets:** Neuroscience data is complex and high-dimensional but contains far fewer and noiser samples than standard ML benchmarks. I believe using existing ML methods in these contexts can help answer scientific questions while also providing unique insight into model failure modes. Currently, I collaborate with the [Hou Lab at CSHL](https://www.houlab.org) to apply 3D pose-tracking models to [study facial expressions in mice](https://www.biorxiv.org/content/10.1101/2024.05.07.593051v1).
2. <img src="/development-ai.png" style="width:250px; margin-left: 2%; border-radius: 5px; float:right">

   **Building ML models inspired by neuronal development and evolution:**
   Deep neural networks are trained on billions of data samples, while biological networks are able to learn within a few training examples by leveraging innate priors encoded in an organism’s genome via evolution. The importance of structural priors is well-known in ML, but a scalable mechanism for learning useful priors does not exist. Biology makes use of two processes for finding innate structure—neuronal development, which translates the information encoded in the genome into a functional network of neurons, and evolution, which mutates the genome to produce better networks. I study these processes through a computational lens to build AI models endowed with [prior structure](https://arxiv.org/abs/2505.22994). In turn, these models are more sample-efficient and consume less energy to train.
<!--3. **Studying neural processing using computer science tools:** Even in a single brain, various sub-circuits employ different computing paradigms, memory systems, information encoding schemes, and signaling strategies. This diversity allows organisms to balance functional specialization and adaptability under various resource constraints. I study these circuits through the lens of a computer scientist to understand how to build heterogenous and distributed computing systems. I believe such systems will be necessary in the post-Moore's law era of computing.-->
